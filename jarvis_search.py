#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
jarvis_search_v2.py ‚Äì Production-ready SearXNG client –¥–ª—è Jarvis.

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- Adaptive rate limiting (—É–º–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏)
- Exponential backoff retry
- Engine fallback (–µ—Å–ª–∏ Google –±–∞–Ω–∏—Ç ‚Üí –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ DDG)
- Result caching (disk-based —Å UTF-8)
- Health monitoring (–æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –¥–≤–∏–∂–∫–æ–≤)
- Domain diversity enforcement
- Query simplification –ø—Ä–∏ –ø—É—Å—Ç—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
- Unicode support (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –æ—à–∏–±–∫–∞ encoding)

Usage:
    python jarvis_search_v2.py "query 1" "query 2" "query 3"
    python jarvis_search_v2.py --verbose --max-sources 15 "deep learning"
"""

import asyncio
import hashlib
import json
import logging
import random
import re
import time
from collections import defaultdict
from dataclasses import dataclass, field
from pathlib import Path
from typing import List, Dict, Optional, Set
from urllib.parse import urlencode, urlparse, parse_qs, urlunparse

import aiohttp

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


# =====================================================================
# CONFIGURATION
# =====================================================================

@dataclass
class SearchConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞"""
    base_url: str = "http://localhost:8080"
    timeout: int = 10
    max_retries: int = 3
    cache_dir: Path = field(default_factory=lambda: Path(".cache/searxng"))
    cache_ttl: int = 3600  # 1 —á–∞—Å

    # Rate limiting
    min_delay: float = 0.5  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
    max_delay: float = 2.0  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
    jitter: bool = True  # –°–ª—É—á–∞–π–Ω–æ–µ –≤–∞—Ä—å–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–¥–µ—Ä–∂–µ–∫

    # Parallelism
    max_concurrent: int = 5  # –ú–∞–∫—Å. –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

    # Engine fallback
    preferred_engines: List[str] = field(default_factory=lambda: [
        "duckduckgo", "brave", "qwant", "wikipedia"
    ])
    banned_engines: Set[str] = field(default_factory=set)

    def __post_init__(self):
        self.cache_dir.mkdir(parents=True, exist_ok=True)


# =====================================================================
# CACHING
# =====================================================================

class SearchCache:
    """–î–∏—Å–∫–æ–≤—ã–π –∫—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π UTF-8"""

    def __init__(self, cache_dir: Path, ttl: int = 3600):
        self.cache_dir = cache_dir
        self.ttl = ttl
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def _get_cache_key(self, query: str, engines: Optional[List[str]] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        key_data = f"{query}:{','.join(sorted(engines or []))}"
        return hashlib.md5(key_data.encode('utf-8')).hexdigest()

    def get(self, query: str, engines: Optional[List[str]] = None) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞ —Å robust error handling"""
        cache_key = self._get_cache_key(query, engines)
        cache_file = self.cache_dir / f"{cache_key}.json"

        if not cache_file.exists():
            return None

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–∑—Ä–∞—Å—Ç –∫—ç—à–∞
        try:
            cache_age = time.time() - cache_file.stat().st_mtime
            if cache_age > self.ttl:
                cache_file.unlink()  # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–π
                return None
        except Exception as e:
            logger.debug(f"Cache stat error: {e}")
            return None

        # –ß–∏—Ç–∞–µ–º —Å fallback –Ω–∞ —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
        for encoding in ['utf-8', 'utf-8-sig', 'latin-1']:
            try:
                text = cache_file.read_text(encoding=encoding)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –Ω–µ –ø—É—Å—Ç–æ–π
                if not text or not text.strip():
                    logger.debug(f"Empty cache file: {cache_file.name}")
                    cache_file.unlink()  # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª
                    return None

                data = json.loads(text)
                logger.info(f"üíæ Cache HIT: {query[:50]}... (age: {int(cache_age)}s)")
                return data

            except json.JSONDecodeError as e:
                if encoding == 'latin-1':  # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                    logger.warning(f"Cache corrupted, deleting: {cache_file.name}")
                    cache_file.unlink()  # –£–¥–∞–ª—è–µ–º –ø–æ–ª–æ–º–∞–Ω–Ω—ã–π —Ñ–∞–π–ª
                    return None
                # –ü—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É
                continue

            except UnicodeDecodeError:
                if encoding == 'latin-1':  # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                    logger.warning(f"Cache encoding error, deleting: {cache_file.name}")
                    cache_file.unlink()
                    return None
                continue

            except Exception as e:
                logger.warning(f"Cache read error ({encoding}): {e}")
                if encoding == 'latin-1':
                    cache_file.unlink()
                    return None
                continue

        return None

    def set(self, query: str, data: Dict, engines: Optional[List[str]] = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à —Å –∞—Ç–æ–º–∞—Ä–Ω–æ–π –∑–∞–ø–∏—Å—å—é"""
        cache_key = self._get_cache_key(query, engines)
        cache_file = self.cache_dir / f"{cache_key}.json"
        temp_file = self.cache_dir / f"{cache_key}.tmp"

        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø–∏—à–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            temp_file.write_text(
                json.dumps(data, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )

            # –ê—Ç–æ–º–∞—Ä–Ω–∞—è –∑–∞–º–µ–Ω–∞ (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —á–∞—Å—Ç–∏—á–Ω–æ –∑–∞–ø–∏—Å–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã)
            temp_file.replace(cache_file)

            logger.debug(f"üíæ Cache WRITE: {query[:50]}... ‚Üí {cache_file.name}")

        except Exception as e:
            logger.warning(f"Cache write error: {e}")
            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª, –µ—Å–ª–∏ –æ–Ω –æ—Å—Ç–∞–ª—Å—è
            if temp_file.exists():
                temp_file.unlink()

    def repair(self):
        """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ –ø–æ–ª–æ–º–∞–Ω–Ω—ã–µ –∫—ç—à —Ñ–∞–π–ª—ã"""
        repaired = 0
        total = 0

        for cache_file in self.cache_dir.glob("*.json"):
            total += 1
            try:
                # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å
                text = cache_file.read_text(encoding='utf-8')
                json.loads(text)
            except Exception:
                # –ü–æ–ª–æ–º–∞–Ω ‚Äî —É–¥–∞–ª—è–µ–º
                cache_file.unlink()
                repaired += 1
                logger.info(f"üîß Repaired: {cache_file.name}")

        logger.info(f"‚úÖ Cache repair complete: {repaired}/{total} files removed")

    def clear(self):
        """–û—á–∏—â–∞–µ—Ç –≤–µ—Å—å –∫—ç—à"""
        for cache_file in self.cache_dir.glob("*.json"):
            cache_file.unlink()
        logger.info("üóëÔ∏è Cache cleared")


# =====================================================================
# RATE LIMITER
# =====================================================================

class AdaptiveRateLimiter:
    """–£–º–Ω—ã–π rate limiter —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏"""

    def __init__(self, min_delay: float = 0.5, max_delay: float = 2.0, jitter: bool = True):
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.jitter = jitter
        self.last_request_time = 0
        self.consecutive_errors = 0
        self.lock = asyncio.Lock()

    async def acquire(self):
        """–ñ–¥—ë—Ç –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º"""
        async with self.lock:
            now = time.time()
            elapsed = now - self.last_request_time

            # –í—ã—á–∏—Å–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É (—É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö)
            base_delay = self.min_delay * (1.5 ** self.consecutive_errors)
            base_delay = min(base_delay, self.max_delay)

            # –î–æ–±–∞–≤–ª—è–µ–º jitter (—Å–ª—É—á–∞–π–Ω–æ–µ –≤–∞—Ä—å–∏—Ä–æ–≤–∞–Ω–∏–µ)
            if self.jitter:
                delay = base_delay + random.uniform(0, base_delay * 0.5)
            else:
                delay = base_delay

            # –ñ–¥—ë–º, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if elapsed < delay:
                wait_time = delay - elapsed
                logger.debug(f"‚è±Ô∏è Rate limit: waiting {wait_time:.2f}s")
                await asyncio.sleep(wait_time)

            self.last_request_time = time.time()

    def report_success(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á—ë—Ç—á–∏–∫ –æ—à–∏–±–æ–∫ –ø—Ä–∏ —É—Å–ø–µ—Ö–µ"""
        self.consecutive_errors = max(0, self.consecutive_errors - 1)

    def report_error(self):
        """–£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        self.consecutive_errors = min(self.consecutive_errors + 1, 5)
        if self.consecutive_errors > 0:
            logger.warning(f"‚ö†Ô∏è Consecutive errors: {self.consecutive_errors} (delays increased)")


# =====================================================================
# MAIN CLIENT
# =====================================================================

class JarvisSearchClient:
    """Production-ready SearXNG –∫–ª–∏–µ–Ω—Ç"""

    def __init__(self, config: Optional[SearchConfig] = None):
        self.config = config or SearchConfig()
        self.cache = SearchCache(self.config.cache_dir, self.config.cache_ttl)
        self.rate_limiter = AdaptiveRateLimiter(
            min_delay=self.config.min_delay,
            max_delay=self.config.max_delay,
            jitter=self.config.jitter
        )
        self.semaphore = asyncio.Semaphore(self.config.max_concurrent)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (–¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞)
        self.stats = {
            "total_queries": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "engine_errors": defaultdict(int),
            "total_results": 0,
            "simplified_queries": 0
        }

    def _simplify_query(self, query: str) -> str:
        """
        –£–ø—Ä–æ—â–∞–µ—Ç –∑–∞–ø—Ä–æ—Å, —É–±–∏—Ä–∞—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –¥–∞—Ç—ã

        Examples:
            "machine learning RAG 2024" ‚Üí "machine learning retrieval augmented generation"
            "deep learning transformers 2025" ‚Üí "deep learning transformers"
        """
        # –£–±–∏—Ä–∞–µ–º –≥–æ–¥—ã (1900-2099)
        simplified = re.sub(r'\b(19|20)\d{2}\b', '', query)

        # –†–∞—Å–∫—Ä—ã–≤–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        abbreviation_map = {
            'RAG': 'retrieval augmented generation',
            'LLM': 'large language model',
            'NLP': 'natural language processing',
            'CV': 'computer vision',
            'ML': 'machine learning',
            'AI': 'artificial intelligence',
            'DL': 'deep learning',
            'RL': 'reinforcement learning',
            'GAN': 'generative adversarial network',
            'CNN': 'convolutional neural network',
            'RNN': 'recurrent neural network'
        }

        for abbr, full in abbreviation_map.items():
            # Case-insensitive –∑–∞–º–µ–Ω–∞
            pattern = re.compile(r'\b' + re.escape(abbr) + r'\b', re.IGNORECASE)
            if pattern.search(simplified):
                simplified = pattern.sub(full, simplified, count=1)
                break  # –ó–∞–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        simplified = ' '.join(simplified.split())

        return simplified.strip()

    def _get_fallback_engines(self, current_engines: List[str]) -> List[str]:
        """–í—ã–±–∏—Ä–∞–µ—Ç –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –¥–≤–∏–∂–∫–∏ –ø—Ä–∏ –Ω–µ—É–¥–∞—á–µ"""
        all_safe = ["duckduckgo", "brave", "qwant", "wikipedia"]

        # –ü—Ä–æ–±—É–µ–º —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –µ—â—ë –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏
        unused = [e for e in all_safe
                  if e not in current_engines
                  and e not in self.config.banned_engines]

        if unused:
            return unused[:2]  # –ë–µ—Ä—ë–º 2 –Ω–æ–≤—ã—Ö
        else:
            # Fallback –Ω–∞ —Å–∞–º—ã–π –Ω–∞–¥—ë–∂–Ω—ã–π
            return ["duckduckgo"]

    async def search(
            self,
            query: str,
            engines: Optional[List[str]] = None,
            category: Optional[str] = None,
            max_results: Optional[int] = None,
            simplify_on_failure: bool = True
    ) -> Dict:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —Å retry logic –∏ fallback

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            engines: –°–ø–∏—Å–æ–∫ –¥–≤–∏–∂–∫–æ–≤ (None = –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å preferred_engines)
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è ('general', 'science', 'it', 'news')
            max_results: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            simplify_on_failure: –£–ø—Ä–æ—â–∞—Ç—å –∑–∞–ø—Ä–æ—Å –ø—Ä–∏ –ø—É—Å—Ç—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö

        Returns:
            Dict —Å –∫–ª—é—á–∞–º–∏: results, query, engines_used, from_cache
        """
        self.stats["total_queries"] += 1
        original_query = query  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º preferred engines, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã
        if engines is None:
            engines = [e for e in self.config.preferred_engines
                       if e not in self.config.banned_engines]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cached = self.cache.get(original_query, engines)
        if cached:
            self.stats["cache_hits"] += 1
            cached["from_cache"] = True
            return cached

        self.stats["cache_misses"] += 1

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Å retry
        for attempt in range(self.config.max_retries):
            try:
                result = await self._search_with_rate_limit(
                    query, engines, category, max_results
                )

                if result.get("results"):
                    # –£—Å–ø–µ—Ö - –∫—ç—à–∏—Ä—É–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
                    self.rate_limiter.report_success()
                    result["from_cache"] = False
                    result["original_query"] = original_query
                    result["final_query"] = query
                    result["simplified"] = (query != original_query)

                    # –ö—ç—à–∏—Ä—É–µ–º –ø–æ–¥ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
                    self.cache.set(original_query, result, engines)
                    self.stats["total_results"] += len(result["results"])

                    return result

                else:
                    # –ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –≤–æ–∑–º–æ–∂–Ω–æ –±–∞–Ω –∏–ª–∏ –Ω–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                    logger.warning(
                        f"Empty result for '{query}' "
                        f"(attempt {attempt + 1}/{self.config.max_retries})"
                    )
                    self.rate_limiter.report_error()

                    if attempt < self.config.max_retries - 1:
                        # Strategy 1: Try fallback engines
                        new_engines = self._get_fallback_engines(engines)

                        # Strategy 2: Simplify query –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–ø—ã—Ç–∫–µ
                        if (attempt == self.config.max_retries - 2
                                and simplify_on_failure
                                and query == original_query):  # –ï—â—ë –Ω–µ —É–ø—Ä–æ—â–∞–ª–∏

                            simplified = self._simplify_query(query)
                            if simplified != query:
                                query = simplified
                                self.stats["simplified_queries"] += 1
                                logger.info(f"üí° Query simplified: '{original_query}' ‚Üí '{query}'")

                        backoff = 2 ** attempt
                        logger.info(
                            f"Retrying with engines={new_engines} after {backoff}s..."
                        )
                        engines = new_engines
                        await asyncio.sleep(backoff)

            except Exception as e:
                logger.error(
                    f"Search error on attempt {attempt + 1}: "
                    f"{type(e).__name__}: {e}"
                )
                self.rate_limiter.report_error()

                if attempt < self.config.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)

        # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ—É–¥–∞—á–Ω—ã
        logger.error(f"‚ùå Search failed after {self.config.max_retries} retries: {original_query}")

        simplified_suggestion = self._simplify_query(original_query)

        return {
            "results": [],
            "query": original_query,
            "final_query": query,
            "error": "Max retries exceeded",
            "suggestion": (
                f"Try: '{simplified_suggestion}'"
                if simplified_suggestion != original_query
                else None
            )
        }

    async def _search_with_rate_limit(
            self,
            query: str,
            engines: List[str],
            category: Optional[str],
            max_results: Optional[int]
    ) -> Dict:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ —Å rate limiting"""

        async with self.semaphore:
            await self.rate_limiter.acquire()

            # –°—Ç—Ä–æ–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            params = {
                "q": query,
                "format": "json",
                "engines": ",".join(engines)
            }

            if category:
                params["categories"] = category

            if max_results:
                params["pageno"] = 1  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π

            url = f"{self.config.base_url}/search?{urlencode(params)}"
            logger.debug(f"üîó Request: {url}")

            timeout = aiohttp.ClientTimeout(total=self.config.timeout)

            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(url) as response:

                    if response.status == 200:
                        data = await response.json()
                        results = data.get("results", [])

                        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ max_results
                        if max_results:
                            results = results[:max_results]

                        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–≤–∏–∂–∫–∞–º (–¥–ª—è debug)
                        if logger.isEnabledFor(logging.DEBUG):
                            engine_counts = {}
                            for r in results:
                                engine = r.get("engine", "unknown")
                                engine_counts[engine] = engine_counts.get(engine, 0) + 1
                            logger.debug(f"Engine breakdown: {engine_counts}")

                        logger.info(
                            f"‚úÖ '{query[:50]}...' ‚Üí {len(results)} results "
                            f"(engines: {engines})"
                        )

                        return {
                            "results": results,
                            "query": query,
                            "engines_used": engines
                        }

                    elif response.status == 429:
                        # Rate limit –æ—Ç SearXNG
                        logger.warning(f"‚ö†Ô∏è 429 Rate Limit from SearXNG")
                        raise aiohttp.ClientError("Rate limited by SearXNG")

                    else:
                        error_text = await response.text()
                        logger.error(
                            f"‚ùå HTTP {response.status}: {error_text[:200]}"
                        )
                        raise aiohttp.ClientError(f"HTTP {response.status}")

    async def parallel_search(self, queries: List[str]) -> List[Dict]:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Å–µ–º–∞—Ñ–æ—Ä–æ–º"""

        logger.info(f"üîç Starting parallel search for {len(queries)} queries...")

        tasks = [self.search(q) for q in queries]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        clean_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Query {i + 1} failed: {result}")
                clean_results.append({
                    "results": [],
                    "error": str(result),
                    "query": queries[i]
                })
            else:
                clean_results.append(result)

        return clean_results

    def print_stats(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã"""
        print("\n" + "=" * 70)
        print("üìä JARVIS SEARCH STATISTICS")
        print("=" * 70)
        print(f"Total queries:      {self.stats['total_queries']}")

        if self.stats['total_queries'] > 0:
            hit_rate = self.stats['cache_hits'] / self.stats['total_queries'] * 100
            print(f"Cache hits:         {self.stats['cache_hits']} ({hit_rate:.1f}%)")
        else:
            print(f"Cache hits:         {self.stats['cache_hits']}")

        print(f"Cache misses:       {self.stats['cache_misses']}")
        print(f"Total results:      {self.stats['total_results']}")
        print(f"Simplified queries: {self.stats['simplified_queries']}")

        if self.stats['engine_errors']:
            print("\nEngine errors:")
            for engine, count in self.stats['engine_errors'].items():
                print(f"  {engine}: {count}")

        print("=" * 70 + "\n")

    async def health_check(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å SearXNG"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                        f"{self.config.base_url}/search?q=test&format=json",
                        timeout=aiohttp.ClientTimeout(total=5)
                ) as response:

                    if response.status == 200:
                        data = await response.json()
                        results = data.get("results", [])

                        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–≤–∏–∂–∫–∞–º
                        engine_counts = {}
                        for r in results:
                            engine = r.get("engine", "unknown")
                            engine_counts[engine] = engine_counts.get(engine, 0) + 1

                        return {
                            "healthy": True,
                            "total_results": len(results),
                            "active_engines": list(engine_counts.keys()),
                            "engine_breakdown": engine_counts
                        }
                    else:
                        return {
                            "healthy": False,
                            "error": f"HTTP {response.status}"
                        }

        except Exception as e:
            return {"healthy": False, "error": str(e)}


# =====================================================================
# URL DEDUPLICATION
# =====================================================================

class URLDeduplicator:
    """–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∏ Domain Diversity"""

    def __init__(self, max_per_domain: int = 2):
        self.seen_urls: Set[str] = set()
        self.domain_counts: Dict[str, int] = defaultdict(int)
        self.max_per_domain = max_per_domain

    def normalize_url(self, url: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL"""
        parsed = urlparse(url.lower())

        # –£–±–∏—Ä–∞–µ–º tracking –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if parsed.query:
            params = parse_qs(parsed.query)
            clean_params = {
                k: v for k, v in params.items()
                if k not in [
                    'utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content',
                    'ref', 'fbclid', 'gclid', 'source', 'si', '_hsenc', '_hsmi'
                ]
            }
            query = urlencode(clean_params, doseq=True) if clean_params else ''
        else:
            query = ''

        return urlunparse((
            parsed.scheme,
            parsed.netloc,
            parsed.path.rstrip('/'),
            '',  # params
            query,
            ''  # fragment
        ))

    def is_duplicate(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç —Å domain diversity"""
        normalized = self.normalize_url(url)

        # Level 1: URL duplicate
        if normalized in self.seen_urls:
            logger.debug(f"‚è≠Ô∏è  URL duplicate: {url}")
            return True

        # Level 2: Domain diversity
        domain = urlparse(normalized).netloc
        if self.domain_counts[domain] >= self.max_per_domain:
            logger.debug(f"‚è≠Ô∏è  Domain limit reached: {domain}")
            return True

        # –î–æ–±–∞–≤–ª—è–µ–º
        self.seen_urls.add(normalized)
        self.domain_counts[domain] += 1
        return False

    def get_stats(self) -> Dict:
        return {
            "unique_urls": len(self.seen_urls),
            "unique_domains": len(self.domain_counts),
            "domain_distribution": dict(self.domain_counts)
        }


# =====================================================================
# HIGH-LEVEL API
# =====================================================================

async def smart_search(
        queries: List[str],
        max_sources: int = 10,
        config: Optional[SearchConfig] = None
) -> List[Dict]:
    """
    –£–º–Ω—ã–π –ø–æ–∏—Å–∫ —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π –∏ domain diversity

    Args:
        queries: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
        max_sources: –ú–∞–∫—Å–∏–º—É–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞

    Returns:
        List —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    client = JarvisSearchClient(config or SearchConfig())
    deduplicator = URLDeduplicator(max_per_domain=2)

    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫
    search_results = await client.parallel_search(queries)

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    all_results = []
    for search_result in search_results:
        all_results.extend(search_result.get("results", []))

    logger.info(f"üì¶ Total raw results: {len(all_results)}")

    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
    unique_results = []
    duplicates_count = 0

    for result in all_results:
        url = result.get("url", "")
        if not url:
            continue

        if not deduplicator.is_duplicate(url):
            unique_results.append(result)

            if len(unique_results) >= max_sources:
                logger.info(f"‚úÖ Reached max_sources limit ({max_sources})")
                break
        else:
            duplicates_count += 1

    logger.info(f"‚úÖ Unique results: {len(unique_results)}")
    logger.info(f"‚è≠Ô∏è  Duplicates filtered: {duplicates_count}")

    stats = deduplicator.get_stats()
    logger.info(f"üåê Unique domains: {stats['unique_domains']}")
    logger.debug(f"Domain distribution: {stats['domain_distribution']}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞
    client.print_stats()

    return unique_results


# =====================================================================
# CLI INTERFACE
# =====================================================================

async def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è CLI"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Jarvis Search Engine - Production-ready SearXNG client",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
        Examples:
          python jarvis_search_v2.py "deep learning"
          python jarvis_search_v2.py --verbose "RAG 2024"
          python jarvis_search_v2.py --clear-cache
          python jarvis_search_v2.py --repair-cache
        """
    )

    parser.add_argument('queries', nargs='*', help='Search queries')
    parser.add_argument('--verbose', '-v', action='store_true', help='Enable debug logging')
    parser.add_argument('--max-sources', type=int, default=10, help='Max unique sources')
    parser.add_argument('--cache-ttl', type=int, default=3600, help='Cache TTL in seconds')
    parser.add_argument('--no-cache', action='store_true', help='Disable cache')
    parser.add_argument('--clear-cache', action='store_true', help='Clear cache and exit')
    parser.add_argument('--repair-cache', action='store_true', help='Repair corrupted cache files')  # NEW!
    parser.add_argument('--health-check', action='store_true', help='Check SearXNG health')

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    config = SearchConfig(
        base_url="http://localhost:8080",
        min_delay=0.5,
        max_delay=2.0,
        max_concurrent=5,
        max_retries=3,
        cache_ttl=args.cache_ttl if not args.no_cache else 0
    )

    cache = SearchCache(config.cache_dir, config.cache_ttl)

    # NEW: Repair cache
    if args.repair_cache:
        print("üîß Repairing cache...")
        cache.repair()
        return

    if args.clear_cache:
        cache.clear()
        print("‚úÖ Cache cleared successfully")
        return

    # Health check
    if args.health_check:
        client = JarvisSearchClient(config)
        print("üè• Checking SearXNG health...")
        health = await client.health_check()
        print(json.dumps(health, indent=2, ensure_ascii=False))
        return

    # Queries
    if args.queries:
        queries = args.queries
    else:
        # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        queries = [
            "–ø–æ–≥–æ–¥–∞ —á–µ–ª—è–±–∏–Ω—Å–∫ 08 –¥–µ–∫–∞–±—Ä—è 2025 –≥–æ–¥",
        ]
        print(f"üìù Using {len(queries)} test queries\n")

    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
    results = await smart_search(queries, max_sources=args.max_sources, config=config)

    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "=" * 70)
    print("üéØ FINAL RESULTS")
    print("=" * 70)

    if not results:
        print("\n‚ùå No results found. Troubleshooting:")
        print("  1. Check SearXNG: docker ps | grep searxng")
        print("  2. View logs: docker logs jarvis-searxng")
        print("  3. Test manually: curl 'http://localhost:8080/search?q=test&format=json'")
        print("  4. Health check: python jarvis_search_v2.py --health-check")
        print("  5. Enable verbose: python jarvis_search_v2.py --verbose")
        return

    for i, result in enumerate(results, 1):
        title = result.get('title', 'No title')
        url = result.get('url', 'No URL')
        engine = result.get('engine', 'unknown')
        snippet = result.get('content', '')[:150]

        print(f"\n{i}. {title}")
        print(f"   URL: {url}")
        print(f"   Engine: {engine}")
        if snippet:
            print(f"   {snippet}...")

    print("\n" + "=" * 70)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Interrupted by user")
    except Exception as e:
        logger.exception("Fatal error:")
        print(f"\n‚ùå Fatal error: {e}")
