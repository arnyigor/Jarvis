# smart_search_pipeline.py
# –°–∏—Å—Ç–µ–º–∞: JarvisSearchV2 (SearXNG) ‚Üí LLM-—Ñ–∏–ª—å—Ç—Ä (GPT-OSS 20B) ‚Üí Async Read ‚Üí LLM Answer.

import asyncio
import json
import logging
import re
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

import aiohttp
import lmstudio as lms  # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π Python SDK LM Studio
import trafilatura
from lmstudio._sdk_models import Temperature

# –ò–ú–ü–û–†–¢–ò–†–£–ï–ú –í–ê–® –ü–†–û–î–í–ò–ù–£–¢–´–ô –ö–õ–ò–ï–ù–¢
try:
    from jarvis_search import smart_search, SearchConfig
except ImportError:
    print("‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª jarvis_search.py. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –æ–Ω –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ.")
    exit(1)

# -----------------------
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
# -----------------------

LMSTUDIO_MODEL_NAME = "gpt-oss-20b"
MAX_RESULTS_FOR_LLM = 20  # –ú–∞–∫—Å–∏–º—É–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è LLM
TOP_K = 3  # –°–∫–æ–ª—å–∫–æ –≤—ã–±—Ä–∞—Ç—å –¥–ª—è —á—Ç–µ–Ω–∏—è
MAX_CHARS_PER_DOC = 4000  # –û–±—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞
FETCH_TIMEOUT = 5.0  # –¢–∞–π–º–∞—É—Ç —á—Ç–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s - %(message)s"
)
logger = logging.getLogger("SmartSearchPipeline")


@dataclass
class SearchResult:
    """–ï–¥–∏–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    id: int
    title: str
    url: str
    snippet: str


# -----------------------
# LLM –ö–ª–∏–µ–Ω—Ç (GPT-OSS 20B)
# -----------------------

class LMStudioClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è LLM —á–µ—Ä–µ–∑ lmstudio SDK."""

    def __init__(self, model_name: str = LMSTUDIO_MODEL_NAME):
        self.model = lms.llm(model_name)
        logger.info(f"LM Studio LLM –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {model_name}")

    def _extract_final_response(self, text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ –º–∞—Ä–∫–µ—Ä–∞ <|channel|>final<|message|>"""
        separator = "<|end|><|start|>assistant<|channel|>final<|message|>"
        if separator in text:
            return text.split(separator, 1)[-1].strip()
        return text

    def _run_chat(self, system_prompt: str, user_prompt: str) -> str:
        """–°—Ç—Ä–∏–º–∏–Ω–≥ —á–∞—Ç–∞ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        chat = lms.Chat(system_prompt)
        chat.add_user_message(user_prompt)

        # Temp 0.1 –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–º–∞
        config = lms.LlmPredictionConfig(temperature=Temperature(0.1))

        try:
            stream = self.model.respond_stream(chat, config=config)
            full_content = ""

            # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –≤–∏–¥–µ—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –≤ –∫–æ–Ω—Å–æ–ª–∏ (–¥–ª—è –¥–µ–±–∞–≥–∞)
            # print("ü§ñ LLM Stream: ", end="", flush=True)

            for chunk in stream:
                if chunk.content:
                    # print(chunk.content, end="", flush=True)
                    full_content += chunk.content

            # print()
            return self._extract_final_response(full_content)

        except Exception as e:
            logger.error(f"LLM Error: {e}")
            return ""

    def _extract_json_ids(self, text: str) -> List[int]:
        """–ù–∞–¥–µ–∂–Ω—ã–π –ø–∞—Ä—Å–µ—Ä JSON-—Å–ø–∏—Å–∫–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞."""

        # 1. –û—á–∏—Å—Ç–∫–∞ –æ—Ç markdown (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if "```" in text:
            text = text.replace("``````", "")

        # 2. –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω (–í–ù–ï –±–ª–æ–∫–∞ if!)
        matches = re.findall(r'\[([\d,\s]+)\]', text)

        candidates = []
        for match in matches:
            try:
                json_str = f"[{match}]"
                parsed = json.loads(json_str)
                if isinstance(parsed, list) and all(isinstance(x, int) for x in parsed):
                    candidates.append(parsed)
            except json.JSONDecodeError:
                continue

        # –¢–µ–ø–µ—Ä—å candidates —Ç–æ—á–Ω–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (–ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –∏–ª–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ)
        return candidates[-1] if candidates else []

    def select_relevant_ids(self, query: str, results: List[SearchResult], top_k: int = TOP_K) -> List[int]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ LLM."""
        if not results:
            return []

        candidates = results[:MAX_RESULTS_FOR_LLM]
        items_text = ""
        for r in candidates:
            items_text += f"[{r.id}] {r.title} | {r.snippet[:120]}...\n"

        system_prompt = (
            "You are a precise search result filter. "
            "Your task is to select only results that are truly helpful for answering the user query. "
            "Do not explain. Do not analyze. "
            "Return output strictly as a JSON list of integers."
        )
        user_prompt = f"QUERY: {query}\n\nCANDIDATES:\n{items_text}\nTASK: Select up to {top_k} most relevant IDs.\nOUTPUT FORMAT: JSON list.\nYOUR OUTPUT:"

        raw_response = self._run_chat(system_prompt, user_prompt)
        ids = self._extract_json_ids(raw_response)

        # –í–∞–ª–∏–¥–∞—Ü–∏—è ID
        valid_ids = {r.id for r in candidates}
        filtered = [i for i in ids if i in valid_ids]

        if not filtered and ids:
            logger.warning(f"LLM –≤–µ—Ä–Ω—É–ª–∞ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ ID: {ids}")

        return filtered

    def answer_with_context(self, query: str, context_blocks: List[Dict[str, Any]]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º."""
        sources_text = ""
        for i, doc in enumerate(context_blocks, 1):
            sources_text += f"---\nSource {i}: {doc['title']}\nURL: {doc['url']}\n\n{doc['text']}\n\n"

        system_prompt = (
            "You are a helpful AI assistant. Answer the user's question using ONLY the provided sources. "
            "Cite sources as [Source N] where appropriate."
        )
        user_prompt = f"USER QUESTION:\n{query}\n\nSOURCES:\n{sources_text}\n***\nProvide a detailed answer in Russian."

        return self._run_chat(system_prompt, user_prompt)


# -----------------------
# –ß—Ç–µ–Ω–∏–µ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
# -----------------------

class WebPageReader:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""

    def __init__(self, timeout: float = FETCH_TIMEOUT, max_chars: int = MAX_CHARS_PER_DOC):
        self.timeout = timeout
        self.max_chars = max_chars
        self._headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0 Safari/537.36"
        }

    async def _fetch_html(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        try:
            async with session.get(url, headers=self._headers, timeout=self.timeout) as resp:
                if resp.status != 200:
                    return None
                return await resp.text(errors='ignore')
        except Exception:
            return None

    def _extract_text(self, html: str) -> Optional[str]:
        try:
            text = trafilatura.extract(html, include_comments=False, include_tables=False, include_links=False)
            if not text:
                return None
            text = " ".join(text.split())
            if len(text) > self.max_chars:
                text = text[:self.max_chars] + "... [truncated]"
            return text
        except Exception:
            return None

    async def fetch_and_extract_many(self, results: List[SearchResult]) -> List[Dict[str, Any]]:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ."""
        extracted_docs: List[Dict[str, Any]] = []
        async with aiohttp.ClientSession() as session:
            tasks = [self._fetch_html(session, r.url) for r in results]
            html_pages = await asyncio.gather(*tasks, return_exceptions=True)

        for r, html in zip(results, html_pages):
            if isinstance(html, Exception) or not html:
                continue
            text = self._extract_text(html)
            if text and len(text) > 100:
                extracted_docs.append({"url": r.url, "title": r.title, "text": text})

        return extracted_docs


# -----------------------
# –û—Å–Ω–æ–≤–Ω–æ–π –ü–∞–π–ø–ª–∞–π–Ω
# -----------------------

class SmartSearchPipeline:
    def __init__(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞—à JarvisSearchV2 –∫–æ–Ω—Ñ–∏–≥
        self.search_config = SearchConfig(
            base_url="http://localhost:8080",
            max_concurrent=5,
            cache_ttl=3600
        )
        self.llm = LMStudioClient()
        self.reader = WebPageReader()

    async def answer_question(self, query: str) -> str:
        logger.info(f"üöÄ –°—Ç–∞—Ä—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏: {query}")

        # 1. –ü–û–ò–°–ö (–ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞—à jarvis_search_v2)
        # smart_search –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤, –ø–æ—ç—Ç–æ–º—É –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ list
        # –û–Ω–∞ —Å–∞–º–∞ –¥–µ–ª–∞–µ—Ç rate limit, cache, deduplication
        raw_results_dicts = await smart_search(
            queries=[query],
            max_sources=20,  # –ë–µ—Ä–µ–º —Å –∑–∞–ø–∞—Å–æ–º –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            config=self.search_config
        )

        if not raw_results_dicts:
            return "–ü–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä–∏ –≤ –æ–±—ä–µ–∫—Ç—ã SearchResult
        search_results = []
        for idx, r in enumerate(raw_results_dicts):
            search_results.append(SearchResult(
                id=idx,
                title=r.get('title', 'No title'),
                url=r.get('url', ''),
                snippet=r.get('content', '')
            ))

        logger.info(f"üîç SearXNG –≤–µ—Ä–Ω—É–ª {len(search_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.")

        # 2. –§–ò–õ–¨–¢–†–ê–¶–ò–Ø (LLM)
        logger.info("üß† LLM –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å...")
        relevant_ids = await asyncio.to_thread(
            self.llm.select_relevant_ids, query, search_results, TOP_K
        )

        if not relevant_ids:
            return "LLM –Ω–µ –Ω–∞—à–ª–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ —Å—Ä–µ–¥–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞."

        selected = [r for r in search_results if r.id in relevant_ids]
        logger.info(f"‚úÖ –í—ã–±—Ä–∞–Ω–æ {len(selected)} —Å—Å—ã–ª–æ–∫: {[r.url for r in selected]}")

        # 3. –ß–¢–ï–ù–ò–ï (Async)
        logger.info("üåê –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        docs = await self.reader.fetch_and_extract_many(selected)

        if not docs:
            return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å —Ç–µ–∫—Å—Ç –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π."

        # 4. –û–¢–í–ï–¢ (LLM)
        logger.info("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞...")
        answer = await asyncio.to_thread(
            self.llm.answer_with_context, query, docs
        )

        return answer


# -----------------------
# CLI –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
# -----------------------

if __name__ == "__main__":
    async def main():
        pipeline = SmartSearchPipeline()
        print("\nü§ñ Jarvis Search Pipeline V2 Ready.")
        print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è jarvis_search_v2 –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ GPT-OSS 20B –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.\n")

        while True:
            try:
                q = input("–í–æ–ø—Ä–æ—Å (–∏–ª–∏ 'exit'): ")
                if q.lower() in ['exit', 'quit']:
                    break

                print("-" * 50)
                answer = await pipeline.answer_question(q)
                print("\n=== –û–¢–í–ï–¢ ===\n")
                print(answer)
                print("-" * 50 + "\n")
            except KeyboardInterrupt:
                break
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞: {e}")


    asyncio.run(main())
