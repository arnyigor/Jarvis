#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Hybrid RAG v5.0 ‚Äì –ó–∞—â–∏—Ç–∞ –æ—Ç –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏, –∞—Ç–æ–º–∞—Ä–Ω–æ—Å—Ç—å –∫–µ—à–∞ –∏ BM25.
"""
import asyncio
import hashlib
import io
import json
import logging
import re
import shutil
import sys
import tempfile
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from multiprocessing import Pool
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

import chromadb
import torch
import trafilatura
from chromadb.config import Settings
from docling.document_converter import DocumentConverter
from scipy.sparse import csr_matrix
from sentence_transformers import SentenceTransformer, CrossEncoder
from tqdm import tqdm
from multiprocessing import Pool, cpu_count

warnings = None  # silence future warnings ‚Äì same behaviour as original

# -------------------------------------------------------------------
# Logging
# -------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('hybrid_rag.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


# –ü—ã—Ç–∞–µ–º—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å pymorphy3, –Ω–æ –¥–µ–ª–∞–µ–º —ç—Ç–æ –≤–Ω—É—Ç—Ä–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏,
# —á—Ç–æ–±—ã –Ω–µ –≥—Ä—É–∑–∏—Ç—å –µ–≥–æ —Å—Ä–∞–∑—É –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è (—Ö–æ—Ç—è —ç—Ç–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ, pymorphy –ª–µ–≥–∫–∏–π)
try:
    import pymorphy3
    PYMORPHY_AVAILABLE = True
except ImportError:
    PYMORPHY_AVAILABLE = False

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –≤–æ—Ä–∫–µ—Ä–∞
_morph_analyzer = None

def init_worker():
    """–≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞"""
    global _morph_analyzer
    if PYMORPHY_AVAILABLE:
        _morph_analyzer = pymorphy3.MorphAnalyzer()

def lemmatize_text_worker(text: str) -> str:
    """–§—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ"""
    global _morph_analyzer
    if _morph_analyzer is None:
        # Fallback, –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –∏–Ω–∏—Ç –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª (—Ö–æ—Ç—è –¥–æ–ª–∂–µ–Ω)
        if PYMORPHY_AVAILABLE:
            _morph_analyzer = pymorphy3.MorphAnalyzer()
        else:
            return text # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å

    # –õ–æ–≥–∏–∫–∞ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ (–∫–æ–ø–∏—Ä—É–µ–º –≤–∞—à—É)
    words = re.findall(r'\b\w+\b', text.lower())
    lemmas = []

    for word in words:
        if word.isdigit(): continue

        # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–µ—Ü–∫–æ–¥ –∏–ª–∏ –∞–Ω–≥–ª - –æ—Å—Ç–∞–≤–ª—è–µ–º
        if re.match(r'[a-z0-9]+', word):
            lemmas.append(word)
            continue

        try:
            # –ë–µ—Ä–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É
            parsed = _morph_analyzer.parse(word)[0]
            lemma = parsed.normal_form
            if re.match(r'^[–∞-—è—ëa-z0-9]+$', lemma, re.IGNORECASE):
                lemmas.append(lemma)
        except:
            lemmas.append(word)

    return " ".join(lemmas)


# -------------------------------------------------------------------
# Utilities
# -------------------------------------------------------------------
def project_root() -> Path:
    return Path(__file__).resolve().parent.parent


def validate_gpu_availability() -> Tuple[bool, str]:
    cuda_available = torch.cuda.is_available()
    print(f"CUDA Available: {cuda_available}")
    if cuda_available:
        print(f"CUDA Version: {torch.version.cuda}")
        print(f"GPU Name: {torch.cuda.get_device_name(0)}")
        device_name = torch.cuda.get_device_name(0)
        vram_total = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3
        return True, f"{device_name} ({vram_total:.1f} GB VRAM)"
    else:
        return False, "CPU only (CUDA not available)"


def compute_file_hash(file_path: Path) -> str:
    sha256_hash = hashlib.sha256()
    try:
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(8192), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    except Exception as e:
        logger.error(f"Failed to hash {file_path}: {e}")
        return ""


def check_disk_space(required_bytes: int, path: Path | str) -> None:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Ö–≤–∞—Ç–∞–µ—Ç –ª–∏ –¥–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏ `required_bytes`.
    –ï—Å–ª–∏ –Ω–µ—Ç ‚Äì –±—Ä–æ—Å–∞–µ–º RuntimeError.
    """
    stat = shutil.disk_usage(str(path))
    available = stat.free
    # 20‚ÄØ% –∑–∞–ø–∞—Å–∞ –Ω–∞ —Å–ª—É—á–∞–π ¬´–ø–∞–º—è—Ç–∏ –ø–æ–¥ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã¬ª
    if available < required_bytes * 1.2:
        raise RuntimeError(
            f"Insufficient disk space: {available / 1e9:.1f}‚ÄØGB free "
            f"(needs ~{required_bytes / 1e9:.1f}‚ÄØGB)"
        )


# -------------------------------------------------------------------
# Lemmatizer
# -------------------------------------------------------------------
try:
    import pymorphy3  # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º pymorphy3 –≤–º–µ—Å—Ç–æ pymorphy2

    PYMORPHY_AVAILABLE = True
except ImportError:
    PYMORPHY_AVAILABLE = False


class RussianLemmatizerFast:
    """–ë—ã—Å—Ç—Ä—ã–π –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –Ω–∞ pymorphy3"""

    def __init__(self):
        if not PYMORPHY_AVAILABLE:
            raise ImportError("pymorphy3 not installed. Install: pip install pymorphy3 pymorphy3-dicts-ru")

        logger.info("üîÑ Initializing pymorphy3 lemmatizer...")
        self.morph = pymorphy3.MorphAnalyzer()
        logger.info("‚úÖ Fast lemmatizer ready")

    def lemmatize(self, text: str) -> List[str]:
        """
        –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ —á–∏—Å–µ–ª.

        Args:
            text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç

        Returns:
            –°–ø–∏—Å–æ–∫ –ª–µ–º–º (—Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞, –±–µ–∑ —á–∏—Å–µ–ª –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏)

        Examples:
            >>> lemmatizer = RussianLemmatizerFast()
            >>> lemmatizer.lemmatize("12345")
            []
            >>> lemmatizer.lemmatize("—Å–ª–æ–≤–æ 123 —Ç–µ–∫—Å—Ç")
            ['—Å–ª–æ–≤–æ', '—Ç–µ–∫—Å—Ç']
        """
        # ‚úÖ –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        words = re.findall(r'\b\w+\b', text.lower())
        lemmas = []

        for word in words:
            # ‚úÖ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∏—Å—Ç—ã–µ —á–∏—Å–ª–∞
            if word.isdigit():
                continue

            # ‚úÖ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–æ–≤–∞ –±–µ–∑ –±—É–∫–≤
            if not re.search(r'[–∞-—è—ëa-z]', word, re.IGNORECASE):
                continue

            try:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π (—Å–∞–º—ã–π –≤–µ—Ä–æ—è—Ç–Ω—ã–π) —Ä–∞–∑–±–æ—Ä
                parsed = self.morph.parse(word)[0]
                lemma = parsed.normal_form

                # ‚úÖ –§–∏–ª—å—Ç—Ä—É–µ–º: —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã (–ë–ï–ó —Ü–∏—Ñ—Ä)
                if re.match(r'^[–∞-—è—ëa-z]+$', lemma, re.IGNORECASE):
                    lemmas.append(lemma)
            except Exception:
                # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å, –±–µ—Ä–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ —Å–ª–æ–≤–æ
                if re.match(r'^[–∞-—è—ëa-z]+$', word, re.IGNORECASE):
                    lemmas.append(word)

        return lemmas

    def lemmatize_batch(self, texts: List[str]) -> List[List[str]]:
        """
        –ë–∞—Ç—á–µ–≤–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è.
        pymorphy3 –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–∞—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á–∏–Ω–≥,
        –Ω–æ –º–æ–∂–Ω–æ —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏—Ç—å —á–µ—Ä–µ–∑ multiprocessing (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ).
        """
        return [self.lemmatize(text) for text in texts]


# -------------------------------------------------------------------
# Configuration
# -------------------------------------------------------------------
@dataclass
class HybridConfig:
    static_docs_dir: Path

    # Models
    embedding_model: str = "intfloat/multilingual-e5-small"
    rerank_model: str = "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1"

    # Text processing
    chunk_size: int = 512
    chunk_overlap: int = 200
    min_chunk_length: int = 100

    # Storage
    chromadb_dir: Path = field(default_factory=lambda: Path("./chromadb"))
    collection_name: str = "jarvis_knowledge"

    # Indexes
    bm25_index_file: Path = field(
        default_factory=lambda: Path("./chromadb/bm25_index.json")
    )
    index_cache_file: Path = field(
        default_factory=lambda: Path("./chromadb/index_cache.json")
    )
    use_lemmatization: bool = True

    # Indexing
    reindex_interval_days: int = 7
    batch_size: int = 100

    # Embedding
    embedding_batch_size: int = 8
    normalize_embeddings: bool = True

    # Query‚Äëtime
    top_k_bm25: int = 50
    top_k_semantic: int = 50
    top_k_final: int = 5
    rerank_threshold: float = 0.0
    exact_match_boost: float = 10.0

    # Performance
    enable_progress_bars: bool = True
    cache_dir: Path = field(default_factory=lambda: Path("./models_cache"))

    def __post_init__(self):
        self.chromadb_dir.mkdir(parents=True, exist_ok=True)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.index_cache_file.parent.mkdir(parents=True, exist_ok=True)
        self.bm25_index_file.parent.mkdir(parents=True, exist_ok=True)


# -------------------------------------------------------------------
# Index Cache with transactions
# -------------------------------------------------------------------
class IndexCache:
    """
    –ö–µ—à –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∞—Ç–æ–º–∞—Ä–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π.
    """

    def __init__(self, cache_file: Path):
        self.cache_file = cache_file
        self.cache: Dict[str, Dict] = {}
        self._load_cache()

    def _load_cache(self):
        if not self.cache_file.exists():
            logger.info("Index cache not found, starting fresh")
            return

        try:
            with open(self.cache_file, "r", encoding="utf-8") as f:
                self.cache = json.load(f)

            if not isinstance(self.cache, dict):
                logger.warning("‚ö†Ô∏è  Invalid cache format, resetting")
                self.cache = {}
            logger.info(f"‚úÖ Index cache loaded: {len(self.cache)} files")
        except Exception as e:
            logger.error(f"‚ùå Failed to load cache: {e}, resetting")
            self.cache = {}

    def _save_cache(self):
        """–ê—Ç–æ–º–∞—Ä–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫—ç—à. –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Å—Ç–æ –ø–µ—Ä–µ–¥ –∑–∞–ø–∏—Å—å—é."""
        # 1Ô∏è‚É£ –ü—Ä–µ–≤—å—é ‚Äì —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É, —á—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å —Ä–∞–∑–º–µ—Ä
        data_str = json.dumps(
            self.cache,
            indent=2,
            ensure_ascii=False
        )
        required_bytes = len(data_str.encode("utf-8"))

        # 2Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤–æ–±–æ–¥–Ω—ã–π –¥–∏—Å–∫ (–ø—É—Ç—å –∫ –∫–∞—Ç–∞–ª–æ–≥—É CROMA)
        check_disk_space(required_bytes, self.cache_file.parent)

        # 3Ô∏è‚É£ –¢–µ–ø–µ—Ä—å –∞—Ç–æ–º–∞—Ä–Ω–∞—è –∑–∞–ø–∏—Å—å –≤ tmp‚Äë—Ñ–∞–π–ª
        try:
            with tempfile.NamedTemporaryFile(
                    mode="w",
                    encoding="utf-8",
                    dir=self.cache_file.parent,
                    delete=False,
                    suffix=".tmp",
            ) as tmp_file:
                tmp_file.write(data_str)
                tmp_path = Path(tmp_file.name)

            # –∞—Ç–æ–º–∞—Ä–Ω–æ–µ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ
            tmp_path.replace(self.cache_file)
            logger.debug(
                f"‚úÖ Index cache saved atomically: {len(self.cache)} files"
            )
        except Exception as e:
            logger.error(f"‚ùå Failed to save cache: {e}")
            if "tmp_path" in locals() and tmp_path.exists():
                tmp_path.unlink()

    def is_indexed(self, file_path: Path, current_hash: str) -> bool:
        key = str(file_path)
        return key in self.cache and self.cache[key].get("hash", "") == current_hash

    def mark_indexed(self, file_path: Path, file_hash: str, chunk_count: int):
        """–û—Ç–º–µ—Ç–∫–∞ —Ñ–∞–π–ª–∞ ‚Äì commit –ø–æ–∑–∂–µ."""
        self.cache[str(file_path)] = {
            "hash": file_hash,
            "chunk_count": chunk_count,
            "indexed_at": datetime.now().isoformat(),
        }

    def commit(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è atomically."""
        self._save_cache()

    def rollback(self):
        """–û—Ç–∫–∞—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π ‚Äì –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ä—É—é –∫–æ–ø–∏—é."""
        logger.warning("‚ö†Ô∏è  Rolling back index cache changes")
        self._load_cache()

    def get_indexed_files(self) -> Set[str]:
        return set(self.cache.keys())

    def clear(self):
        self.cache = {}
        self._save_cache()


# -------------------------------------------------------------------
# BM25 Index with transactions
# -------------------------------------------------------------------
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  BM25‚ÄëANN –∏–Ω–¥–µ–∫—Å (TF‚ÄëIDF + FAISS)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

import faiss
import joblib
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer


class BM25ANNIndex:
    """
    –•—Ä–∞–Ω–∏–ª–∏—â–µ —Å ¬´BM25‚Äë–ø–æ–¥–æ–±–Ω–æ–π¬ª —ç–≤—Ä–∏—Å—Ç–∏–∫–æ–π, –Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∫–∞–∫ ANN.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç TF‚ÄëIDF‚Äë–≤–µ–∫—Ç–æ—Ä—ã –∏ FAISS (inner‚Äëproduct) –¥–ª—è O(log‚ÄØn) –ø–æ–∏—Å–∫–∞.
    """

    def __init__(self,
                 index_file: Path,
                 use_lemmatization: bool = True):
        self.index_file = index_file
        self.use_lemmatization = use_lemmatization

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –∏ FAISS‚Äë–∏–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –≤ –ø–∞–º—è—Ç—å
        self.vectorizer: Optional[TfidfVectorizer] = None
        self.faiss_index: Optional[faiss.Index] = None
        self.corpus_ids: List[str] = []

        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä (–µ—Å–ª–∏ –≤–∫–ª—é—á—ë–Ω)
        self.lemmatizer = RussianLemmatizerFast() if use_lemmatization else None

        self._load_index()

    def _reset_index(self):
        logger.warning("üóëÔ∏è  –°–±—Ä–∞—Å—ã–≤–∞–µ–º –ø–æ–≤—Ä–µ–∂–¥—ë–Ω–Ω—ã–π TF‚ÄëIDF/FAISS –∏–Ω–¥–µ–∫—Å")
        self.vectorizer = None
        self.faiss_index = None
        self.corpus_ids.clear()

    def _load_index(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã (vectorizer + FAISS) –∏–∑ disk."""
        if not self.index_file.exists():
            logger.info("BM25‚ÄëANN –∏–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, –±—É–¥–µ—Ç –ø–æ—Å—Ç—Ä–æ–µ–Ω –ø—Ä–∏ –ø–µ—Ä–≤–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
            return

        try:
            data = joblib.load(str(self.index_file))
            self.vectorizer = data["vectorizer"]
            self.faiss_index = data["faiss_index"]
            self.corpus_ids = data.get("corpus_ids", [])
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∑–∏–ª–∏ TF‚ÄëIDF/FAISS –∏–Ω–¥–µ–∫—Å: {len(self.corpus_ids)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        except Exception as e:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å BF‚ÄëANN –∏–Ω–¥–µ–∫—Å: {e}")
            self._reset_index()

    def _save_index(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º vectorizer + FAISS. –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Å—Ç–æ."""
        # –°–Ω–∞—á–∞–ª–∞ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –≤—Å—ë –≤ –ø–∞–º—è—Ç—å, —á—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å —Ä–∞–∑–º–µ—Ä
        buf = io.BytesIO()
        joblib.dump(
            {
                "vectorizer": self.vectorizer,
                "faiss_index": self.faiss_index,
                "corpus_ids": self.corpus_ids,
            },
            buf,
        )
        required_bytes = buf.tell()  # –¥–ª–∏–Ω–∞ —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞

        check_disk_space(required_bytes, self.index_file.parent)

        try:
            with tempfile.NamedTemporaryFile(
                    mode="wb",
                    dir=self.index_file.parent,
                    delete=False,
                    suffix=".tmp",
            ) as tmp:
                tmp.write(buf.getvalue())
                tmp_path = Path(tmp.name)

            tmp_path.replace(self.index_file)
            logger.info("‚úÖ TF‚ÄëIDF/FAISS –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω")
        except Exception as e:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é: {e}")

    # ------------------------------------------------------------------
    def build_index(self, documents: List[Dict]):
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –∏ FAISS‚Äë–∏–Ω–¥–µ–∫—Å–∞ –∏–∑ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        –° –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û–ô –õ–ï–ú–ú–ê–¢–ò–ó–ê–¶–ò–ï–ô.
        """
        # 1Ô∏è‚É£ –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ ids
        raw_texts = [doc["text"] for doc in documents] # –ò—Å—Ö–æ–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        self.corpus_ids = [doc["id"] for doc in documents]

        # 2Ô∏è‚É£ –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è (Pre-processing)
        raw_texts = [doc["text"] for doc in documents]

        logger.info("üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è BM25...")

        if self.use_lemmatization:
            logger.info(f"‚ö° –ó–∞–ø—É—Å–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ ({len(raw_texts)} docs)...")

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤!
            # –ù–∞ Windows –ª—É—á—à–µ –Ω–µ –∂–∞–¥–Ω–∏—á–∞—Ç—å. 4-6 –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ.
            # –ï—Å–ª–∏ –ø–æ—Å—Ç–∞–≤–∏—Ç—å cpu_count(), –ø–∞–º—è—Ç—å –∫–æ–Ω—á–∏—Ç—Å—è.
            num_workers = min(6, cpu_count())

            try:
                # –í–ê–ñ–ù–û: –ø–µ—Ä–µ–¥–∞–µ–º initializer, —á—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å MorphAnalyzer –æ–¥–∏–Ω —Ä–∞–∑ –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å
                with Pool(processes=num_workers, initializer=init_worker) as pool:
                    processed_texts = pool.map(lemmatize_text_worker, raw_texts)

            except Exception as e:
                logger.error(f"‚ùå Multiprocessing failed: {e}. Fallback to serial.")
                # Fallback: –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–∫–∞–ª—å–Ω–æ –∏ –¥–µ–ª–∞–µ–º –≤ —Ü–∏–∫–ª–µ
                init_worker()
                processed_texts = [lemmatize_text_worker(t) for t in raw_texts]
        else:
            processed_texts = raw_texts

        # 3Ô∏è‚É£ TF‚ÄëIDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (–£–∂–µ –Ω–∞ –≥–æ—Ç–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö)
        logger.info("üîÑ TF‚ÄëIDF‚Äë–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è...")

        # –í–ê–ñ–ù–û:
        # 1. tokenizer=None, preprocessor=None -> –º—ã —É–∂–µ –≤—Å—ë —Å–¥–µ–ª–∞–ª–∏ —Å–∞–º–∏
        # 2. token_pattern=r"(?u)\b\w+\b" -> —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π, —Ä–∞–∑–±–∏–≤–∞–µ—Ç –ø–æ –ø—Ä–æ–±–µ–ª–∞–º (—Ç–æ, —á—Ç–æ –Ω–∞–º –Ω–∞–¥–æ)
        # –ò–õ–ò token_pattern=None + tokenizer=lambda x: x.split()

        # –°–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± –¥–ª—è –ø—Ä–µ-–ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ "word1 word2":
        self.vectorizer = TfidfVectorizer(
            token_pattern=r"(?u)\b\w\w+\b", # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω (—Å–ª–æ–≤–∞ –æ—Ç 2 –±—É–∫–≤)
            lowercase=True # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π, —Ö–æ—Ç—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä —É–∂–µ low
        )

        tfidf_matrix: csr_matrix = self.vectorizer.fit_transform(processed_texts)

        # 4Ô∏è‚É£ –°–æ–∑–¥–∞—ë–º FAISS‚Äë–∏–Ω–¥–µ–∫—Å
        dim = tfidf_matrix.shape[1]
        logger.info(f"üîß –ö–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ–º FAISS –∏–Ω–¥–µ–∫—Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ {dim}")
        index_flat: faiss.Index = faiss.IndexFlatIP(dim)
        self.faiss_index = index_flat

        self.faiss_index.add(tfidf_matrix.toarray())
        logger.info(f"‚úÖ FAISS –≥–æ—Ç–æ–≤ ‚Äì {len(self.corpus_ids)} –≤–µ–∫—Ç–æ—Ä–æ–≤")

        # 5Ô∏è‚É£ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self._save_index()

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Å–∞ (–∏–ª–∏ –≤–Ω–µ –∫–ª–∞—Å—Å–∞) –¥–ª—è Pool.map
    def _lemmatize_text_helper(self, text):
        # –û–±–µ—Ä—Ç–∫–∞, —á—Ç–æ–±—ã –≤—ã–∑—ã–≤–∞—Ç—å –º–µ—Ç–æ–¥ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä–∞
        return self.lemmatizer.lemmatize(text)


    # ------------------------------------------------------------------
    def search(self,  query: str, top_k: int = 10) -> List[Tuple[str, float]]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É —Å –ø–æ–º–æ—â—å—é FAISS –∏ TF‚ÄëIDF.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (doc_id, score), –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —É–±—ã–≤–∞–Ω–∏—é.
        """
        if self.faiss_index is None or not self.corpus_ids:
            logger.warning("FAISS –∏–Ω–¥–µ–∫—Å –ø—É—Å—Ç ‚Äì –≤–æ–∑–≤—Ä–∞—â–∞–µ–º []")
            return []

        # 1Ô∏è‚É£ –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å
        query_vec: np.ndarray = self.vectorizer.transform([query]).toarray()

        # 2Ô∏è‚É£ ANN –ø–æ–∏—Å–∫
        distances, indices = self.faiss_index.search(query_vec, top_k)
        results: List[Tuple[str, float]] = []

        for dist, idx in zip(distances[0], indices[0]):
            if idx < 0:  # FAISS –∏–Ω–æ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç -1
                continue
            doc_id = self.corpus_ids[idx]
            results.append((doc_id, float(dist)))

        return results

    # ------------------------------------------------------------------
    def clear(self):
        """–ü–æ–ª–Ω–æ—Å—Ç—å—é —É–¥–∞–ª—è–µ–º –∏–Ω–¥–µ–∫—Å (–≤ –ø–∞–º—è—Ç—å –∏ —Å –¥–∏—Å–∫–∞)."""
        self._reset_index()
        if self.index_file.exists():
            try:
                self.index_file.unlink()
                logger.info("‚ùå –£–¥–∞–ª—ë–Ω TF‚ÄëIDF/FAISS —Ñ–∞–π–ª")
            except Exception as e:
                logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª –∏–Ω–¥–µ–∫—Å–∞: {e}")


# -------------------------------------------------------------------
# Integrity Checker
# -------------------------------------------------------------------
class IntegrityChecker:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É ChromaDB, BM25 –∏ IndexCache.
    –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.
    """

    @staticmethod
    def check_consistency(
            chroma_count: int,
            bm25_count: int,
            cache_files: int,
    ) -> Tuple[bool, str]:
        if chroma_count == 0 and bm25_count == 0 and cache_files == 0:
            return True, "Empty state (all zeros) - OK"

        max_discrepancy = 0.1  # 10%

        if bm25_count > 0 and chroma_count > 0:
            ratio = abs(bm25_count - chroma_count) / max(bm25_count, chroma_count)
            if ratio > max_discrepancy:
                return False, f"BM25/ChromaDB mismatch: {bm25_count} vs {chroma_count}"

        if cache_files > 0 and chroma_count == 0:
            return False, f"Cache has {cache_files} files but ChromaDB is empty"

        return True, "Consistency check passed"

    @staticmethod
    def suggest_recovery(
            chroma_count: int,
            bm25_count: int,
            cache_files: int,
    ) -> str:
        if chroma_count == 0 and (bm25_count > 0 or cache_files > 0):
            return "‚ö†Ô∏è  Detected incomplete indexing. Run with force=True to rebuild."

        if bm25_count == 0 and chroma_count > 0:
            return "‚ö†Ô∏è  BM25 index missing. It will be rebuilt automatically."

        if abs(bm25_count - chroma_count) > 100:
            return "‚ö†Ô∏è  Significant mismatch. Consider force=True to rebuild."

        return "‚úÖ No recovery needed"


# -------------------------------------------------------------------
# Hybrid RAG System
# -------------------------------------------------------------------
class HybridRAGSystem:
    """Hybrid RAG —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–π."""

    def __init__(self, config: HybridConfig):
        self.config = config
        self._last_index_time: Optional[datetime] = None

        # Init components
        self.index_cache = IndexCache(config.index_cache_file)
        self.bm25_index = BM25ANNIndex(
            config.bm25_index_file, use_lemmatization=config.use_lemmatization
        )

        cuda_available, device_info = validate_gpu_availability()
        self.device = "cuda" if cuda_available else "cpu"
        logger.info(device_info)

        # Load models
        try:
            self.embedding_model = SentenceTransformer(
                config.embedding_model,
                cache_folder=str(config.cache_dir),
                device=self.device,
            )
            logger.info("‚úÖ Embedding model loaded")
        except Exception as e:
            logger.error(f"‚ùå Embedding model failed: {e}")
            raise

        try:
            self.reranker = CrossEncoder(
                config.rerank_model, device=self.device
            )
            logger.info("‚úÖ Reranker loaded")
        except Exception as e:
            logger.error(f"‚ùå Reranker failed: {e}")
            raise

        # ChromaDB client
        try:
            self.chroma_client = chromadb.PersistentClient(
                path=str(config.chromadb_dir),
                settings=Settings(anonymized_telemetry=False, allow_reset=True),
            )
            self.collection = self.chroma_client.get_or_create_collection(
                name=config.collection_name,
                metadata={"hnsw:space": "cosine"},
            )
            chroma_count = self.collection.count()
            logger.info(f"‚úÖ ChromaDB ready (docs={chroma_count})")
        except Exception as e:
            logger.error(f"‚ùå ChromaDB init failed: {e}")
            raise

        # Consistency check at startup
        self._check_integrity()

    def _check_integrity(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π BM25ANNIndex"""
        chroma_count = self.collection.count()

        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ BM25 –∏–Ω–¥–µ–∫—Å–∞
        if hasattr(self.bm25_index, 'corpus_ids'):
            bm25_count = len(self.bm25_index.corpus_ids)
        elif hasattr(self.bm25_index, 'corpus_texts'):
            bm25_count = len(self.bm25_index.corpus_texts)
        else:
            bm25_count = 0

        cache_files = len(self.index_cache.get_indexed_files())

        is_consistent, msg = IntegrityChecker.check_consistency(
            chroma_count, bm25_count, cache_files
        )
        if is_consistent:
            logger.info(f"‚úÖ Integrity check: {msg}")
        else:
            logger.warning(f"‚ö†Ô∏è  Integrity check FAILED: {msg}")
            recovery_msg = IntegrityChecker.suggest_recovery(
                chroma_count, bm25_count, cache_files
            )
            logger.warning(recovery_msg)

    # -------------------------------------------------------------------
    # Indexing
    # -------------------------------------------------------------------
    def index_static_documents(self, force: bool = False) -> int:
        """
        –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–π.
        –í—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–º–º–∏—Ç—è—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Ü–µ. –ü—Ä–∏ –æ—à–∏–±–∫–µ ‚Äì –æ—Ç–∫–∞—Ç.
        """
        if not force and self._last_index_time:
            elapsed = datetime.now() - self._last_index_time
            if elapsed < timedelta(days=self.config.reindex_interval_days):
                logger.info(
                    f"‚è≠Ô∏è  Skipping reindex (last {elapsed.days}d ago)"
                )
                return 0

        if force:
            logger.info("üóëÔ∏è  Force reindex: clearing...")
            try:
                self.chroma_client.delete_collection(self.config.collection_name)
                self.collection = self.chroma_client.get_or_create_collection(
                    name=self.config.collection_name,
                    metadata={"hnsw:space": "cosine"},
                )
                self.index_cache.clear()
                self.bm25_index.clear()
            except Exception as e:
                logger.error(f"Failed to clear: {e}")

        logger.info(f"üìö Indexing from {self.config.static_docs_dir}")

        documents: List[Dict] = []
        doc_id_start = self.collection.count()
        doc_id = doc_id_start

        stats = {
            "total_files": 0,
            "new_files": 0,
            "skipped": 0,
            "failed": 0,
            "chunks": 0,
        }

        file_list = list(self.config.static_docs_dir.rglob("*"))
        file_list = [
            f
            for f in file_list
            if f.is_file() and f.suffix in {".md", ".txt", ".html", ".pdf"}
        ]

        stats["total_files"] = len(file_list)

        if not file_list:
            logger.warning("‚ö†Ô∏è  No files found")
            return 0

        try:
            progress = tqdm(
                file_list,
                desc="Processing",
                disable=not self.config.enable_progress_bars,
            )

            for file_path in progress:
                try:
                    progress.set_postfix({"file": file_path.name[:30]})
                    file_hash = compute_file_hash(file_path)
                    if not file_hash:
                        stats["failed"] += 1
                        continue

                    if not force and self.index_cache.is_indexed(
                            file_path, file_hash
                    ):
                        stats["skipped"] += 1
                        continue

                    # extract content
                    if file_path.suffix == ".pdf":
                        content = self._extract_pdf_docling(file_path)
                    else:
                        content = self._extract_html_or_text(file_path)

                    if not content or len(content) < self.config.min_chunk_length:
                        stats["skipped"] += 1
                        continue

                    chunks = self._chunk_text(content, file_path.name)

                    if not chunks:
                        stats["skipped"] += 1
                        continue

                    for chunk in chunks:
                        documents.append(
                            {
                                "id": str(doc_id),
                                "text": chunk["text"],
                                "source": str(file_path.relative_to(self.config.static_docs_dir)),
                                "chunk_index": chunk["index"],
                                "file_hash": file_hash,
                                "metadata": {
                                    "source_type": "static",
                                    "indexed_at": datetime.now().isoformat(),
                                    "file_type": file_path.suffix,
                                    "file_hash": file_hash,
                                },
                            }
                        )
                        doc_id += 1

                    # mark in cache (but not commit yet)
                    self.index_cache.mark_indexed(file_path, file_hash, len(chunks))
                    stats["new_files"] += 1
                    stats["chunks"] += len(chunks)

                except Exception as e:
                    stats["failed"] += 1
                    logger.error(f"Error processing {file_path.name}: {e}")

            if not documents:
                logger.info("‚ö†Ô∏è  No new documents")
                return 0

            # embeddings
            logger.info(f"üîÑ Embedding {len(documents)} chunks...")

            all_texts = [doc["text"] for doc in documents]
            embeddings = self.embedding_model.encode(
                all_texts,
                batch_size=self.config.embedding_batch_size,
                show_progress_bar=self.config.enable_progress_bars,
                convert_to_numpy=True,
                normalize_embeddings=self.config.normalize_embeddings,
            )

            # store to chromadb
            logger.info("üíæ Storing in ChromaDB...")
            self._add_documents_in_batches(documents, embeddings)

            # BM25 index
            logger.info("üîÑ Building BM25 index...")
            self.bm25_index.build_index(documents)

            # commit all changes atomically
            logger.info("‚úÖ Committing changes...")
            self.index_cache.commit()

            self._last_index_time = datetime.now()

            logger.info(f"‚úÖ Indexing complete!")
            logger.info(
                f"   Files: {stats['new_files']} new, "
                f"{stats['skipped']} skipped, {stats['failed']} failed"
            )
            logger.info(f"   Chunks: {stats['chunks']} added")

            return len(documents)

        except KeyboardInterrupt:
            logger.warning("‚ö†Ô∏è  Indexing interrupted by user!")
            logger.warning("üîÑ Rolling back changes...")
            self.index_cache.rollback()
            raise

        except Exception as e:
            logger.error(f"‚ùå Indexing failed: {e}")
            logger.warning("üîÑ Rolling back changes...")
            self.index_cache.rollback()
            raise

    # -------------------------------------------------------------------
    def _add_documents_in_batches(
            self,
            documents: List[Dict],
            embeddings,
            batch_size: Optional[int] = None,
    ):
        if batch_size is None:
            batch_size = self.config.batch_size

        total = len(documents)
        for i in range(0, total, batch_size):
            batch_docs = documents[i: i + batch_size]
            batch_embeddings = embeddings[i: i + batch_size]

            try:
                self.collection.add(
                    ids=[doc["id"] for doc in batch_docs],
                    embeddings=batch_embeddings.tolist(),
                    documents=[doc["text"] for doc in batch_docs],
                    metadatas=[
                        {
                            **doc["metadata"],
                            "source": doc["source"],
                            "chunk_index": doc["chunk_index"],
                        }
                        for doc in batch_docs
                    ],
                )
            except Exception as e:
                logger.error(f"Failed to add batch: {e}")
                raise

    # -------------------------------------------------------------------
    def _extract_pdf_docling(self, file_path: Path) -> Optional[str]:
        try:
            converter = DocumentConverter()
            doc = converter.convert(str(file_path))
            text_parts = [block.text for block in doc.blocks if hasattr(block, "text") and block.text]
            return "\n\n".join(text_parts) if text_parts else None
        except Exception:
            return None

    # -------------------------------------------------------------------
    def _extract_html_or_text(self, file_path: Path) -> Optional[str]:
        try:
            if file_path.suffix == ".html":
                html = file_path.read_text(encoding="utf-8", errors="ignore")
                return trafilatura.extract(html)
            else:
                return file_path.read_text(encoding="utf-8", errors="ignore")
        except Exception as e:
            logger.error(f"Error reading {file_path}: {e}")
            return None

    # -------------------------------------------------------------------
    def _chunk_text(self, text: str, source: str) -> List[Dict]:
        chunks = []
        words = text.split()
        chunk_size_words = int(self.config.chunk_size / 1.3)
        overlap_words = int(self.config.chunk_overlap / 1.3)

        for i in range(0, len(words), chunk_size_words - overlap_words):
            chunk_words = words[i: i + chunk_size_words]
            chunk_text = " ".join(chunk_words)

            if len(chunk_text) >= self.config.min_chunk_length:
                chunks.append({"text": chunk_text, "index": len(chunks)})

        return chunks

    # -------------------------------------------------------------------
    # Search helpers
    # -------------------------------------------------------------------
    def _search_bm25(self, query: str) -> List[Dict]:
        logger.info("üîé BM25‚ÄëANN search‚Ä¶")
        try:
            hits = self.bm25_index.search(query, top_k=self.config.top_k_bm25)
        except Exception as e:
            logger.error(f"BM25‚ÄëANN failed: {e}")
            return []

        if not hits:
            return []

        # –ø–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ Chroma –ø–æ ids
        doc_ids = [hit[0] for hit in hits]
        bm25_scores = {hit[0]: hit[1] for hit in hits}

        results_dict = self.collection.get(ids=doc_ids)

        parsed = []
        for i, doc_id in enumerate(results_dict["ids"]):
            parsed.append(
                {
                    "id": doc_id,
                    "text": results_dict["documents"][i],
                    "metadata": results_dict["metadatas"][i],
                    "bm25_score": bm25_scores.get(doc_id, 0.0),
                    "search_type": "bm25",
                }
            )
        return parsed

    def _search_semantic(self, query_embedding) -> List[Dict]:
        logger.info("üîé Semantic search‚Ä¶")
        try:
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=self.config.top_k_semantic,
            )
            parsed = []
            for i, doc_id in enumerate(results["ids"][0]):
                parsed.append(
                    {
                        "id": doc_id,
                        "text": results["documents"][0][i],
                        "metadata": results["metadatas"][0][i],
                        "distance": (
                            results["distances"][0][i]
                            if "distances" in results
                            else None
                        ),
                        "search_type": "semantic",
                    }
                )
            return parsed
        except Exception as e:
            logger.error(f"Semantic search failed: {e}")
            return []

    def _reciprocal_rank_fusion(
            self,
            bm25_results: List[Dict],
            semantic_results: List[Dict],
            k: int = 60
    ) -> List[Dict]:
        """RRF —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π"""
        logger.info("üîÑ Applying Reciprocal Rank Fusion...")

        rrf_scores: Dict[str, float] = {}
        documents: Dict[str, Dict] = {}

        # BM25 —Ä–∞–Ω–≥–∏
        for rank, doc in enumerate(bm25_results, start=1):
            doc_id = doc["id"]
            rrf_scores[doc_id] = rrf_scores.get(doc_id, 0.0) + (1.0 / (k + rank))

            # ‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ + –ø–æ–º–µ—á–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            if doc_id not in documents:
                documents[doc_id] = doc
                documents[doc_id]["sources"] = ["bm25"]
            else:
                documents[doc_id]["sources"].append("bm25")

        # Semantic —Ä–∞–Ω–≥–∏
        for rank, doc in enumerate(semantic_results, start=1):
            doc_id = doc["id"]
            rrf_scores[doc_id] = rrf_scores.get(doc_id, 0.0) + (1.0 / (k + rank))

            if doc_id not in documents:
                documents[doc_id] = doc
                documents[doc_id]["sources"] = ["semantic"]
            else:
                documents[doc_id]["sources"].append("semantic")

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
        sorted_ids = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)

        combined = []
        top_n_candidates = 60  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–µ —á–∏—Å–ª–æ –¥–ª—è RTX 5070

        for doc_id in sorted_ids[:top_n_candidates]:
            # -------------------------------------------------------------------------
            doc = documents[doc_id]
            doc["rrf_score"] = rrf_scores[doc_id]
            doc["source_type"] = "static"

            # ‚úÖ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–∏–ø–∞: –µ—Å–ª–∏ –≤ –æ–±–æ–∏—Ö, —Ç–æ "hybrid"
            if len(doc["sources"]) > 1:
                doc["search_type"] = "hybrid"
            else:
                doc["search_type"] = doc["sources"][0]

            combined.append(doc)

        logger.info(f"   RRF combined: {len(combined)} unique docs")

        return combined

    def _rerank_results(self, query: str, results: List[Dict]) -> List[Dict]:
        """
        –ü–µ—Ä–µ–Ω–∞—Ä—è–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø–æ–º–æ—â—å—é Cross‚ÄëEncoder –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –±–æ–Ω—É—Å–∞
        –∑–∞ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ. –ü—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ exact‚Äëmatch –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –µ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏–µ
        –≤ —Ç–æ–ø‚Äëk (–µ—Å–ª–∏ –µ—Å—Ç—å). –í —Å–ª—É—á–∞–µ –æ—Ç–∫–∞–∑–∞ —Ä–∞–Ω–∂–µ—Ä–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫.
        """
        if not results:
            return []

        # ---------- 1Ô∏è‚É£ –ü–æ–¥–∞—ë–º –ø–∞—Ä—ã (query, doc) –≤ Cross‚ÄëEncoder ----------
        pairs = [[query, res["text"][:512]] for res in results]
        try:
            raw_scores = self.reranker.predict(pairs)
        except Exception as e:
            logger.error(f"Cross‚ÄëEncoder —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ø–∞–ª–æ: {e}")
            # fallback ‚Äì –æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è
            return sorted(
                results,
                key=lambda r: r.get("bm25_score", 0),
                reverse=True,
            )[: self.config.top_k_final]

        query_lower = query.lower()
        best_exact_score = None

        # ---------- 2Ô∏è‚É£ –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –±–æ–Ω—É—Å ----------
        for res, base in zip(results, raw_scores):
            base_score: float = float(base)
            has_exact = query_lower in res["text"].lower()

            if has_exact:
                qlen = len(query_lower)
                bonus = (
                    2.0 if qlen < 10
                    else 5.0 if qlen < 30
                    else 8.0
                )
                final_score = base_score + bonus
            else:
                final_score = base_score

            res.update(
                {
                    "rerank_score_base": base_score,
                    "has_exact_match": has_exact,
                    "rerank_score": final_score,
                }
            )

            if has_exact and (best_exact_score is None or final_score > best_exact_score):
                best_exact_score = final_score

        # ---------- 3Ô∏è‚É£ –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–º—É –±–∞–ª–ª—É ----------
        sorted_res = sorted(results, key=lambda r: r["rerank_score"], reverse=True)

        # ---------- 4Ô∏è‚É£ Threshold guard ‚Äì –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –ø–æ—è–≤–ª–µ–Ω–∏–µ exact match ----------
        if best_exact_score is not None:
            idx_best = next(
                i for i, r in enumerate(sorted_res) if r["has_exact_match"]
            )
            if idx_best >= self.config.top_k_final:
                # –ø–æ–¥–Ω–∏–º–∞–µ–º score —á—É—Ç—å –≤—ã—à–µ —Ç–µ–∫—É—â–µ–≥–æ k‚Äë–≥–æ
                kth_score = sorted_res[self.config.top_k_final - 1]["rerank_score"]
                sorted_res[idx_best]["rerank_score"] = kth_score + 0.05

        # ---------- 5Ô∏è‚É£ –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ç–±–æ—Ä–∫–∞ ----------
        final_top = sorted(
            sorted_res,
            key=lambda r: r["rerank_score"],
            reverse=True,
        )[: self.config.top_k_final]

        return final_top

    async def search_web_fresh(self, query: str) -> List[Dict]:
        logger.info(f"üåê Fresh web search: '{query}'")
        import aiohttp

        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                        f"{self.config.searxng_url}/search",
                        params={"q": query, "format": "json"},
                        timeout=aiohttp.ClientTimeout(total=self.config.searxng_timeout),
                ) as resp:
                    if resp.status != 200:
                        return []
                    data = await resp.json()
        except Exception as e:
            logger.error(f"SearXNG error: {e}")
            return []

        raw_results = data.get("results", [])[:10]
        extracted_docs = []

        for result in raw_results:
            try:
                html = trafilatura.fetch_url(result["url"])
                if not html:
                    continue
                text = trafilatura.extract(html)
                if text and len(text) > 100:
                    extracted_docs.append(
                        {
                            "text": text[:5000],
                            "url": result["url"],
                            "title": result.get("title", ""),
                            "source_type": "fresh",
                        }
                    )
            except Exception:
                continue

        logger.info(f"‚úì Extracted {len(extracted_docs)} fresh documents")
        return extracted_docs

    async def hybrid_search(self, query: str, use_fresh: bool = False) -> Dict:
        logger.info(f"üîç Hybrid search: '{query[:50]}...'")

        async def bm25_task():
            return self._search_bm25(query)

        async def semantic_task():
            emb = self.embedding_model.encode(
                query,
                normalize_embeddings=self.config.normalize_embeddings,
            )
            return self._search_semantic(emb)

        tasks = [bm25_task(), semantic_task()]
        if use_fresh:
            tasks.append(self.search_web_fresh(query))

        results = await asyncio.gather(*tasks, return_exceptions=True)

        bm25_res = results[0] if not isinstance(results[0], Exception) else []
        sem_res = results[1] if not isinstance(results[1], Exception) else []
        fresh_res = (
            results[2]
            if len(results) > 2 and not isinstance(results[2], Exception)
            else []
        )

        logger.info(f"   BM25: {len(bm25_res)}, Semantic: {len(sem_res)}, Fresh: {len(fresh_res)}")

        combined = self._reciprocal_rank_fusion(bm25_res, sem_res)

        for res in fresh_res:
            combined.append({**res, "source_type": "fresh"})

        logger.info(f"üìä Reranking {len(combined)} documents‚Ä¶")
        ranked = self._rerank_results(query, combined)

        return {
            "query": query,
            "results": ranked,
            "bm25_count": len(bm25_res),
            "semantic_count": len(sem_res),
            "fresh_count": len(fresh_res),
            "total": len(ranked),
            "timestamp": datetime.now().isoformat(),
        }

    # -------------------------------------------------------------------
    def get_collection_stats(self) -> Dict:
        return {
            "name": self.config.collection_name,
            "count": self.collection.count(),
            "last_indexed": (
                self._last_index_time.isoformat()
                if self._last_index_time
                else None
            ),
            "cached_files": len(self.index_cache.get_indexed_files()),
            "bm25_docs": len(self.bm25_index.corpus_ids),
        }

    def reset_collection(self):
        logger.warning("üóëÔ∏è  Resetting collection")
        self.chroma_client.delete_collection(self.config.collection_name)
        self.collection = self.chroma_client.get_or_create_collection(
            name=self.config.collection_name, metadata={"hnsw:space": "cosine"}
        )
        self.index_cache.clear()
        self.bm25_index.clear()
        self._last_index_time = None


# -------------------------------------------------------------------
# Demo entry point
# -------------------------------------------------------------------
async def main():
    config = HybridConfig(
        static_docs_dir=project_root() / "docs",
        embedding_model="intfloat/multilingual-e5-small",
        rerank_model="cross-encoder/mmarco-mMiniLMv2-L12-H384-v1",
        chromadb_dir=project_root() / "chromadb",
        enable_progress_bars=True,
    )

    logger.info("=" * 80)
    logger.info("HYBRID RAG SYSTEM v5.0 ‚Äì BM25 + –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è")
    logger.info("=" * 80)

    system = HybridRAGSystem(config)

    # Step 1: Indexing
    logger.info("\n" + "=" * 80)
    logger.info("STEP 1: INDEXING")
    logger.info("=" * 80)

    indexed = system.index_static_documents(force=False)
    if indexed:
        stats = system.get_collection_stats()
        logger.info(
            f"\nüìä Stats:\n   ChromaDB: {stats['count']} docs\n"
            f"   BM25: {stats['bm25_docs']} docs\n   Cached: {stats['cached_files']} files"
        )

    # Step 2: Demo
    logger.info("\n" + "=" * 80)
    logger.info("STEP 2: HYBRID SEARCH DEMO")
    logger.info("=" * 80)

    queries = [
        "–ß—Ç–æ –±—É–¥–µ—Ç —á–µ—Ä–µ–∑ –ø—è—Ç—å –ª–µ—Ç",
        "–∫–Ω—è–∂–Ω–∞ –ú–∞—Ä—å—è –º–µ—á—Ç–∞–ª–∞, –∫–∞–∫ –º–µ—á—Ç–∞—é—Ç –≤—Å–µ–≥–¥–∞ –¥–µ–≤—É—à–∫–∏",
    ]
    for q in queries:
        logger.info(f"\n{'=' * 60}\nüîç Query: '{q}'\n{'=' * 60}")
        res = await system.hybrid_search(q, use_fresh=False)
        logger.info(
            f"\nüìä Results: {res['total']} documents\n"
            f"   BM25:{res['bm25_count']}\n   Semantic:{res['semantic_count']}\n   Fresh:{res['fresh_count']}"
        )
        # –í —Ñ—É–Ω–∫—Ü–∏–∏ main()
        # –í main():
        for i, doc in enumerate(res['results'][:5], 1):
            base_score = doc.get('rerank_score_base', 0)
            final_score = doc.get('rerank_score', 0)
            rrf_score = doc.get('rrf_score', 0)
            has_exact = doc.get('has_exact_match', False)
            search_type = doc.get('search_type', 'unknown')

            # –ú–∞—Ä–∫–µ—Ä—ã
            exact_marker = " üéØ EXACT" if has_exact else ""
            hybrid_marker = " üîÄ HYBRID" if search_type == "hybrid" else ""

            logger.info(f"\n{i}. [Rerank {final_score:.3f}] [RRF {rrf_score:.4f}]{exact_marker}{hybrid_marker}")

            if has_exact and base_score != final_score:
                bonus = final_score - base_score
                logger.info(f"   (Base: {base_score:.3f} ‚Üí Boosted: +{bonus:.1f})")

            logger.info(f"   Source: {doc.get('metadata', {}).get('source', 'unknown')}")
            logger.info(f"   Type: {search_type}")
            logger.info(f"   Text: {doc['text'][:150]}...")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è  Interrupted")
    except Exception as e:
        logger.error(f"\n‚ùå Fatal error: {e}", exc_info=True)
        sys.exit(1)
